{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"Welcome to the NCAR HPC User Group (NHUG)! <p>NHUG is a dedicated community that aims to promote the productive use of high-performance computing (HPC) facilities at NCAR and increase collaboration among all of the NCAR HPC community.</p> <p>NHUG holds monthly meetings featuring different HPC-related topics, on the third Tuesday of the month, from 1 p.m. to 2 p.m. Mountain Time. The goal of these meetings is to foster a community around HPC-related issues and increase collaboration among all of the NCAR HPC user community. To learn more about NHUG monthly meetings, please visit the NHUG Monthly Meetings page.</p> <p>  \u2002 Monthly Meetings </p> <p>NHUG Communication Channels</p> <p>Join the NHUG email list to receive calendar invites for upcoming monthly meetings and other important announcements. We also encourage you to join the NCAR HPC User Group Slack channel, which we will use for focused interactions during and outside the monthly meetings.  </p> <p> \u2002 Join NHUG Email List   \u2002 Join NHUG Slack Channel</p> <p>Contribute to Future NHUG Topics</p> <p>If you have suggestions for future meeting topics or ideas that can enhance our community, we'd love to hear from you. Please submit your ideas using the form below to help shape our upcoming sessions.</p> <p>  \u2002 Submit Your Ideas</p>"},{"location":"blog/","title":"Blog","text":"RSS Feed <p>NHUG Blog is a location for the NHUG community to share their experiences and knowledge with the rest of the community.</p> <p>This blog post provides a detailed description of the process of creating a new blog post.</p>"},{"location":"blog/welcome-to-the-nhug-blog/","title":"Welcome to the NCAR HPC User Group (NHUG) Blog!","text":"<p>Today, we are excited to announce the launch of the NCAR HPC User Group (NHUG) Blog. This space is dedicated to sharing insights, tips, and tricks about NCAR HPC resources and practices. </p> <p>This blog post explains how to contribute a blog article to the NHUG Blog.</p>"},{"location":"blog/welcome-to-the-nhug-blog/#contributing-to-the-nhug-blog","title":"Contributing to the NHUG Blog","text":"<p>This blog post explains how to contribute a blog article to the NHUG Blog:</p> <ol> <li> <p>Fork the NHUG repository: If you haven't already, fork the NHUG GitHub repository. This step is necessary for first-time contributors.</p> </li> <li> <p>Clone the forked repository and create a new branch from the <code>main</code> branch.</p> </li> <li> <p>Create a file under <code>/docs/blog/posts/</code> with the following naming convention: <code>YYYY-MM-DD-&lt;your-post-title&gt;.md</code>. The filename you choose does'nt affect the URL or title of your post. The URL of your post will be generated based on the <code>title</code> field in the front matter of your post.</p> </li> </ol> <p><pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 blog/\n\u2502     \u251c\u2500 posts/\n\u2502     \u2502  \u2514\u2500 hello-world.md\n|     |  \u2514\u2500 YYYY-MM-DD-your-post-title.md\n\u2502     \u251c\u2500 .authors.yml\n\u2502     \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre> 4. Start your blog post with a YAML front matter section. The front matter section must include the following fields:</p> <pre><code>```yaml\n---\ntitle: &lt;your-post-title&gt;\ndescription: &lt;your-post-description&gt;\ndraft: false\ndate: YYYY-MM-DD\ncategories:\n  - &lt;category-1&gt;\n  - &lt;category-2&gt;\n\n---\n```\n\nThe `title` field is the title of your post. The `description` field is a short description of your post. The `draft` field indicates whether your post is a draft or not. If you set `draft` to `true`, your post will not be published. The `date` field is the date of your post. The `categories` field is a list of categories that your post belongs to. [This page](https://squidfunk.github.io/mkdocs-material/plugins/blog/#metadata) provide a list of other metdata available for each post.\n\nPrepare the rest of your blog post in Markdown format. You can use the [Markdown Cheatsheet](https://www.markdownguide.org/cheat-sheet/) as a reference.\n</code></pre> <ol> <li> <p>Once you are happy with your blog post, commit your changes and push your branch to your forked repository. Then, open a pull request to merge your branch into the <code>main</code> branch of the NHUG repository.</p> </li> <li> <p>Once your pull request is merged, your blog post will be published on the NHUG Blog. Time to celeberate! </p> </li> </ol>"},{"location":"blog/welcome-to-the-nhug-blog/#additional-tips","title":"Additional Tips","text":""},{"location":"blog/welcome-to-the-nhug-blog/#adding-an-excerpt","title":"Adding an Excerpt","text":"<p>To create a concise preview of your post in listings, insert <code>&lt;!-- more --&gt;</code> where you want the excerpt to end:</p> <pre><code>Intro paragraph(s) of your post.\n\n&lt;!-- more --&gt;\n\nContinuation of your post...\n</code></pre>"},{"location":"blog/welcome-to-the-nhug-blog/#adding-an-author","title":"Adding an Author","text":"<p>To add an author to your post, add the following field to the front matter section of your post:</p> <pre><code>authors:\n  - &lt;author-1-identifier&gt;\n  - &lt;author-2-identifier&gt;\n</code></pre> <p>For first-time contributors, add your details to <code>.authors.yml</code> under <code>/docs/blog/</code> using a unique identifier (like your GitHub username). Once you have added your name to the <code>.authors.yml</code> file, you can add your identifier in the <code>authors</code> field in the front matter section of your post.</p>"},{"location":"blog/welcome-to-the-nhug-blog/#blog-post-preview","title":"Blog Post Preview","text":"<p>To preview your blog post locally, first install the following dependencies: <pre><code>pip install -r requirements.txt\n</code></pre> Then,run the following command from the root of the NHUG repository to preview your blog post: <pre><code>mkdocs serve\n</code></pre></p> <p>Stay tuned for more posts, and once again, welcome to the NHUG Blog!</p>"},{"location":"blog/streamlining-two-factor-authentication-with-ssh/","title":"Streamlining two-factor authentication with <code>ssh</code>","text":"<p>Do you tire of responding to Duo pushes every single time you log in to NCAR's HPC resources?  While two-factor authentication (2FA) is a hard requirement for user access, there are techniques for \"sharing\" authentication, allowing multiple <code>ssh</code> or <code>scp</code> processes to reuse a pre-established connection.  This technique uses the <code>ControlPath</code>  and <code>ControlMaster</code> functionality built in to <code>ssh</code> and requires some straightforward local configuration.</p> <p><code>ControlPath</code> and <code>ControlMaster</code> are <code>ssh</code> client options that work together along with an automatically generated socket file, allowing an initial <code>ssh</code> connection to be reused and optionally persist after the initial session has disconnected. In practice this means the first connection to a host will be authenticated normally, but subsequent connections will simply reuse the initial connection without requiring additional authentication.</p>"},{"location":"blog/streamlining-two-factor-authentication-with-ssh/#local-configuration","title":"Local configuration","text":"<p>Nothing special is required on the HPC systems to enable this functionality, however you will need to perform some local configuration of your <code>ssh</code> client.  In the example below I'm using the default <code>OpenSSH</code> on my Macbook laptop, and all that is required is editing the <code>config</code> file in my <code>~/.ssh</code> directory:</p> ~/.ssh/config<pre><code>host casper\n     HostName casper.hpc.ucar.edu\n\nhost derecho\n     HostName derecho.hpc.ucar.edu\n\nhost *\n    ControlMaster auto\n    ControlPath ~/.ssh/controlpath-%r@%h:%p\n    ControlPersist 12h\n    ServerAliveInterval 120s\n    ServerAliveCountMax 5\n</code></pre> <p>The first two sections are simply convenience: they define aliases for the host names <code>casper</code> and <code>derecho</code> to their fully-qualified domain names.  The idea then is I can <code>ssh casper</code> without needing to use the full host and domain name.</p> <p>The final section (<code>host *</code>) contains the specific configuration of interest:</p> <ul> <li> <p><code>ControlPath ~/.ssh/controlpath-%r@%h:%p</code> specifies the path to a controling file \"socket\" used for connection sharing.  This makes use of some special tokens:</p> <ul> <li><code>%r</code> : username on the remote system,</li> <li><code>%h</code> : the remote system host name, and</li> <li><code>%p</code> : the remote port.</li> </ul> <p>This allows for a unique <code>ControlPath</code> file to be placed in our <code>~/.ssh/</code> directory for each unique user/host/port combination.</p> </li> <li> <p><code>ControlMaster auto</code> enables \"opportunistic <code>ssh</code> multiplexing,\" meaning <code>ssh</code> will attempt to use an existing established connection if possible, and establish a new one if required.</p> </li> <li> <p><code>ControlPersist 12h</code> specifies that the controlling connection should remain open in the background (waiting for future client connections) after the initial client connection has been closed.</p> <p>Without this option, closing a controlling <code>ssh</code> session will abruptly terminate any other active, shared authentication connections.</p> </li> <li> <p>The final two lines, <code>ServerAliveInterval 120s</code> and <code>ServerAliveCountMax 5</code>, are useful for maintaining <code>ssh</code> connections.</p> <p>When our client is idle, eventually we will be disconnected from the server.  These settings send very minimal traffic at intervals even when we are not actively using <code>ssh</code>, triggering the server to respond.  If the server does not respond after <code>ServerAliveCountMax</code> steps, our client will finally give up and disconnect.</p> <p>See here for additional discussion.</p> </li> </ul> <p>Your <code>~/.ssh/config</code> file supports many, many more options.  See here for additional details.</p>"},{"location":"blog/streamlining-two-factor-authentication-with-ssh/#demonstration","title":"Demonstration","text":"<p>To see how these pieces work together, consider the following examples:</p> <p>Connecting to <code>casper</code> through a new <code>ControlPath</code> &amp; examining the mechanics of the process</p> <pre><code>ssh-client(1)$ ls ~/.ssh/control*\nls: /Users/benkirk/.ssh/control*: No such file or directory\n\nssh-client(2)$ ssh casper uptime\n(benkirk@casper.hpc.ucar.edu) ncar-two-factor:\n 07:53:13  up 22 days 12:30,  89 users,  load average: 7.74, 7.87, 8.09\n\nssh-client(3)$ file ~/.ssh/control*\n/Users/benkirk/.ssh/controlpath-benkirk@casper.hpc.ucar.edu:22: socket\n\nssh-client(4)$ ps aux | grep \"ssh: \" | grep -v grep | awk '{print $11 \" \" $12 \" \" $13}'\nssh: /Users/benkirk/.ssh/controlpath-benkirk@casper.hpc.ucar.edu:22 [mux]\n\nssh-client(5)$ ssh casper uptime\n 07:53:34  up 22 days 12:31,  87 users,  load average: 7.66, 7.85, 8.08\n</code></pre> <p>Detailed Discussion</p> <ol> <li>For demonstration purposes, we begin on a quiet client with no existing <code>ControlPath</code> instances, as shown in line 1.</li> <li> <p>On line 4 we start a new <code>ssh</code> session to <code>casper</code> to execute a remote command (<code>uptime</code>). You can see from the <code>ncar-two-factor:</code> prompt we are required to two-factor authenticate with Duo, as usual.</p> <p>Also, note that we were able to reference the short host name <code>casper</code> since our <code>~/.ssh/config</code> has this aliased to <code>casper.hpc.ucar.edu</code>.</p> </li> <li> <p>Lines 8-9 show that <code>ssh</code> has now automatically created the <code>~/.ssh/controlpath-benkirk@casper.hpc.ucar.edu:22</code> socket for us, as specified by the <code>ControlPath</code> configuration.</p> </li> <li>Lines 11-12 demonstrate the impact of the <code>ControlPersist</code> option.  Even though we are not currently using <code>ssh</code> (our connection in step 2 has terminated), we still have an <code>ssh</code> process active in the background referencing our <code>ControlPath</code> file.  This process keeps the connection active and ready to be reused by other processes, up to the <code>ControlPersist</code> timeout.</li> <li>In line 14 we repeat the <code>ssh casper uptime</code> command, and no 2FA is required!</li> </ol> <p>The same process applies when adding a new connection to Derecho as seen in the following example.</p> <p>Additionally connecting to <code>derecho</code> through another <code>ControlPath</code></p> <pre><code>ssh-client(6)$ ssh derecho \"uname -a\"\nAccess to and use of this UCAR computer system is limited to authorized use by\nUCAR Policies 1-7 and 3-6 and all applicable federal laws, executive orders,\npolicies and directives. UCAR computer systems are subject to monitoring at all\ntimes to ensure proper functioning of equipment and systems including security\ndevices, to prevent unauthorized use and violations of statutes and security\nregulations, to deter criminal activity, and for other similar purposes.\n\nUsers should be aware that information placed in the system is subject to\nmonitoring and is not subject to any expectation of privacy. Unauthorized use\nor abuse will be dealt with according to UCAR Policy, up to and including\ncriminal or civil penalties as warranted.\n\nBy logging in, you are agreeing to these terms.\n\n(benkirk@derecho.hpc.ucar.edu) ncar-two-factor:\nLinux derecho4 5.14.21-150400.24.18-default #1 SMP PREEMPT_DYNAMIC Thu Aug 4 14:17:48 UTC 2022 (e9f7bfc) x86_64 x86_64 x86_64 GNU/Linux\n\nssh-client(7)$ file ~/.ssh/control*\n/Users/benkirk/.ssh/controlpath-benkirk@casper.hpc.ucar.edu:22:  socket\n/Users/benkirk/.ssh/controlpath-benkirk@derecho.hpc.ucar.edu:22: socket\n\nssh-client(8)$ ps aux | grep \"ssh: \" | grep -v grep | awk '{print $11 \" \" $12 \" \" $13}'\nssh: /Users/benkirk/.ssh/controlpath-benkirk@derecho.hpc.ucar.edu:22 [mux]\nssh: /Users/benkirk/.ssh/controlpath-benkirk@casper.hpc.ucar.edu:22 [mux]\n\nssh-client(9)$ ssh derecho \"uname -a\"\nLinux derecho4 5.14.21-150400.24.18-default #1 SMP PREEMPT_DYNAMIC Thu Aug 4 14:17:48 UTC 2022 (e9f7bfc) x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <p>Detailed Discussion</p> <ol> <li>In lines 1-16 we similarly start a new <code>ssh</code> session to <code>derecho</code> to execute a remote command (<code>uname -a</code>), using the short host name alias.</li> <li>Lines 19-25 demonstrate that we now have a <code>ControlPath</code> and persistent connection to Derecho as well.</li> <li>Lines 27-28 show that subsequent <code>ssh</code> connections do not require 2FA. Success!!</li> </ol> <p>In the examples above we have used <code>ssh</code> to run a remote command, but the same functionality applies with terminal sessions, and extends to <code>scp</code> and <code>sftp</code> as well.</p> <p>Happy <code>ssh</code>'ing!!</p>"},{"location":"blog/unique-bash-history-on-different-hpc-resources/","title":"Unique <code>bash</code> <code>history</code> on different HPC resources","text":"<p>NCAR's HPC resource share a common home files system, which greatly simplifies working across systems.  One slight annoyance I noted with this setup, however, is a common <code>bash</code> history across all machines.  While the commands I usually run on Casper and Derecho are very similar, they are not identical.  Fortunately, <code>bash</code> allows its <code>history</code> behavior to be customized easily, and with just a little effort we can get HPC resource-specific <code>history</code> implementations.</p>"},{"location":"blog/unique-bash-history-on-different-hpc-resources/#customizing-your-bash-history-behavior","title":"Customizing your <code>bash</code> history behavior","text":"<p>By default, <code>bash</code> stores its <code>history</code> in the file <code>~/.bash_history</code>, so shells on different machines will share the same history file.  It's really easy to change this behavior with just a little shell customization.</p> <p>The following snippet is lifted from my <code>~/.profile</code>, where I do all my shell customization:</p>"},{"location":"blog/unique-bash-history-on-different-hpc-resources/#unique-histfile-per-ncar-resource-type","title":"Unique <code>HISTFILE</code> per NCAR resource type","text":"~/.profile<pre><code># increase the number of lines stored in our history file\n# (https://www.gnu.org/software/bash/manual/html_node/Bash-History-Facilities.html)\nexport HISTSIZE=1024\n\n# customize history behavior\n# (https://www.bashsupport.com/bash/variables/bash/histcontrol/)\nexport HISTCONTROL=ignorespace:erasedups\n\n# Machine-specific history file\ncase \"x${NCAR_HOST}\" in\n    \"x\") ;;\n    *) export HISTFILE=$HOME/.bash_history.${NCAR_HOST}; ;;\nesac\n</code></pre> <p>The first two <code>export</code> statements modify the default behavior of the <code>history</code> built-in command, as described in the references.</p> <p>The <code>case</code> switch statement on lines 10-13 examines the value of the <code>NCAR_HOST</code> environment variable and (if set) uses it to create a unique <code>HISTFILE</code>.  <code>NCAR_HOST</code> is set by all our <code>ncarenv</code> modules, so will exist on Derecho and Casper login and compute nodes.</p> <p>With the configuration above, all my <code>casper</code> and <code>derecho</code> commands stay put and each machine has its own distinct <code>history</code>!</p>"},{"location":"blog/unique-bash-history-on-different-hpc-resources/#bonus-search-behavior-customization","title":"Bonus: search behavior customization","text":"<p><code>bash</code> used the GNU readline library for interacting with the user, which allows for a lot of behavior customization.  One feature I found that I like to use under <code>bash</code> is:</p> <ul> <li> <p><code>history-search-forward ()</code></p> <p>Search forward through the history for the string of characters between the start of the current line and the point. The search string must match at the beginning of a history line.</p> </li> <li> <p><code>history-search-backward ()</code></p> <p>Search backward through the history for the string of characters between the start of the current line and the point. The search string must match at the beginning of a history line.</p> </li> </ul> <p>which can be enabled via:</p> ~/.profile<pre><code># substring search history\nbind '\"\\e[A\"':history-search-backward\nbind '\"\\e[B\"':history-search-forward\n</code></pre> <p>What this means is, say I type <code>$ qs</code> at a terminal prompt and then hit the \"up\" arrow, my shell will cycle through my history for any commands beginning with <code>qs*</code>, e.g. <code>qsub</code>, <code>qstat</code>.</p> <p>For other options (and there are many...), see this page.</p>"},{"location":"blog/jupyter-for-vscode/","title":"Using VSCode for Jupyter Notebooks","text":"<p>VSCode is an extensible code editing software that can connect to remote systems like Derecho and Casper.  Intellisense, Jupyter, and other extensions gives you options to keep the editor as lightweight or complex as you prefer.  Additional quality of life features like GUI file transfers, a terminal window, and local or remote Jupyter notebook execution make VSCode an appealing editor for use with NCAR systems.</p> <p>For NCAR, you can use VSCode as a less feature rich alternative to Jupyterhub.  It doesn't have all of the functionality of Jupyterhub but can run the same notebooks with kernels that are available on Derecho, Casper, or your own custom environments.</p> <p>VSCode is available for NCAR issued laptops in the Self Service application.  You can find more information about VSCode and download it here: VSCode</p>"},{"location":"blog/jupyter-for-vscode/#extensions","title":"Extensions","text":"<p>The Jupyter extension is required to preview and run notebooks in VSCode.  You can access the extension library by selecting the Extensions tab on the left side of VSCode.  Click on install and then restart VSCode after completion to run Jupyter Notebooks in VSCode.</p> <p></p> <p>You will also need to install the ipykernel extension to run cells within a Jupyter notebook.  It can be downloaded using the same method as the Jupyter extension or you will be prompted to install it the first time you run a cell in a notebook.</p>"},{"location":"blog/jupyter-for-vscode/#connecting-to-derecho","title":"Connecting to Derecho","text":"<p>On the bottom left corner of VSCode is a &gt;&lt; button that allows connection to a remote host.  A new dropdown will appear and you can select Connect to Host to connect to a remote host like Derecho or Casper.  Connecting to a host pulls information from your local SSH configuration file and you may already have Derecho and Casper as options to connect to.</p> <p></p> <p>Alternatively, when you are prompted to connect to a host you can enter the full SSH path like you would when connecting via the terminal:</p> <pre><code>ssh &lt;user&gt;@derecho.hpc.ucar.edu\n</code></pre> <p>This will connect you to a Derecho login node and you can open a terminal by selecting the menu's Terminal dropdown and clicking New Terminal.  </p>"},{"location":"blog/jupyter-for-vscode/#connecting-to-glade","title":"Connecting to GLADE","text":"<p>If you prefer to use a GUI File Explorer then you can connect to the GLADE filesystem using the tabs of the left side of VSCode.  The Explorer tab prompts you to Open Folder, and if you are already connected to Derecho, then will autocomplete GLADE paths.</p> <p></p> <p>Warning</p> <p>Only open small folders to improve performance and reduce memory load on Derecho login nodes.  VSCode will scan the entire contents of the directory at high frequency causing slow autocomplete for Intellisense and flag you for login resource abuse.</p> <p>Tip</p> <p>You can connect to recently used remote paths by creating a new window and selecting it from the Recent list.</p>"},{"location":"blog/jupyter-for-vscode/#terminal-vs-file-explorer","title":"Terminal vs. File Explorer","text":"<p>Once you have connected to your GLADE path then you can work out of the VSCode File Explorer tab or the terminal.  You can create a new terminal by selecting the Terminal dropdown and clicking New Terminal. </p> <p></p> <p>Info</p> <p>You can use the File Explorer to transfer files between your local machine and Derecho or Casper.  Right click on the file and select Download.</p>"},{"location":"blog/jupyter-for-vscode/#conda-kernels-and-notebooks","title":"Conda, Kernels, and Notebooks","text":"<p>Open a Jupyter notebook using either the File Explorer or the terminal.  For the File Explorer, you can simply click on the filename of the notebook.  Opening in terminal can use the code application:</p> <pre><code>code &lt;path&gt;/my_notebook.ipynb\n</code></pre> <p>This will put you in the default kernel.  Like Jupyterhub, you will need to specify which kernel to use when executing cells in the notebook.  You need to select a kernel from within the Jupyter notebook or it will use your default kernel.</p> <p></p> <p>Adding your conda-envs folder to the list of Trusted kernels will allow you to run notebooks with these kernels without being prompted to trust the kernel each time you run select a new kernel.  Add the path by selecting Settings from the Code dropdown.  In the Settings window search for kernel and add your user path to the Trusted kernel paths.  Your custom environments should be located here:</p> <pre><code>/glade/work/$USER/conda-envs/\n</code></pre> <p></p>"},{"location":"blog/jupyter-for-vscode/#running-jupyter-notebooks","title":"Running Jupyter Notebooks","text":"<p>You can run cells similarly to Jupyterhub with the 'Run All' or an individual cell by hitting Shift+Enter.</p> <p></p> <p>Be mindful that you will be running your notebooks on the login nodes.  Computationally or memory intensive notebooks should be run within Jupyterhub as a batch job.  We are exploring options for running notebooks inside of PBS jobs and hope to have an solution for using VSCode on the compute nodes in the near future.</p>"},{"location":"blog/energy-accounting-on-derecho/","title":"Energy Accounting on Derecho","text":"<p>As part of CISL's recent submission to the Green500 list, we have implemented a PBS hook (plugin) which allows PBS to record cumulative energy consumption data for each job individually. </p> <p>This works by sampling the energy consumed by each node individually since last boot time at both the beginning of the job and the end of the job. The total difference between these two readings is then stored in PBS after the job ends. As a result, energy data displayed while a job is running is not meaningful but will be correct after the job is finished.</p> <p>To query this energy data about a recently finished job (currently within the last 72 hours) <code>qstat -f -x</code> can be used, for example:</p> <pre><code>matthews@derecho1:~&gt; qstat -f -x 4238938 | grep 'x-ncar.*-energy'\n    resources_used.x-ncar-cpu-energy = 758\n    resources_used.x-ncar-energy = 2527\n    resources_used.x-ncar-gpu0-energy = 0\n    resources_used.x-ncar-gpu1-energy = 0\n    resources_used.x-ncar-gpu2-energy = 0\n    resources_used.x-ncar-gpu3-energy = 0\n    resources_used.x-ncar-memory-energy = 1131\n</code></pre> <p>Each of these fields is in Joules (Watt*Seconds) and covers a period of time from immediately before the job script starts to immediately after it ends. Total energy is provided as \"x-ncar-energy\" and the energy that the system attributed to the CPU, RAM, and individual GPUs are called out separately. Note that these major components do not constitute all of the energy consumed by the node so the total energy will generally be higher than the sum of the components. There also may be a small error introduced due to the delay between when each of these measurements is sampled. If a job spans multiple nodes, each field represents the total energy consumed by that device type across all nodes. For example, a three-node GPU job would report the sum of all GPU-ID 0 devices in the \"x-ncar-gpu0-energy\" field.</p> <p>If the job ran on a node which didn't have GPUs (as above), the GPU energy will be listed as 0 Joules. If the job ran on a shared node (the develop queue), it's not possible to completely isolate the energy consumption of your job from other jobs running on that node. So it's recommended to rerun your job on an exclusive use node before trusting this data.</p> <p>For completed jobs which began after approximately April 2 2024, <code>qhist</code> can instead be used. For example, to see all power metrics for a single job:</p> <pre><code>vanderwb@derecho2:~&gt; qhist -j 4246835 -l\n4246835.desched1\n...\n   Node Eng (J)  = 5787\n   CPU  Eng (J)  = 691\n   RAM  Eng (J)  = 1006\n   GPU0 Eng (J)  = 626\n   GPU1 Eng (J)  = 610\n   GPU2 Eng (J)  = 623\n   GPU3 Eng (J)  = 612\n...\n</code></pre> <p>Using typical <code>qhist</code> flags, you can also show power metrics for all of your jobs for a particular time period. Here, we display jobs from April 20, 2024, sorted by node energy usage:</p> <pre><code>vanderwb@derecho2:~&gt; qhist -p 20240420 -f '{energy-node}' -s energy-node | head -n 6\nJob ID    E-Node(J)\n4190307   6005563096\n4198428   5612065907\n4194550   3745933887\n4199951   3336310504\n4197478   1259626184\n</code></pre> <p>It should also be noted that this data includes only the energy consumed by the compute nodes. Energy consumed by the interconnect, control system, cooling, and storage is not included as it would be very difficult to proportion to individual jobs. In CISL's experiments for the Green500 it was observed that the GPU partition consumes approximately 15% more energy measured at the circuit breaker than would be accounted for by this method. Energy dedicated to storage and generating facility chilled water would be additional to that 15% but it does include the part of the interconnect which is local to the racks in question and some of the control system. Nevertheless, this data should be accurate enough for use in comparisons of workloads on a science throughput per Joule basis.</p> <p>Due to the heterogeneous and shared nature of Casper, this feature is currently only available on Derecho; however, if you would find it useful on Casper, please let us know. </p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/","title":"qvscode","text":"<p>NCAR CISL Consulting Services Group has added the qvscode script that starts a new VSCode session on a Casper compute node.  The script can either take user input via terminal prompts or source a settings file to allocate resources for a compute node job.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#motivation","title":"Motivation","text":"<p>VSCode uses significant resources on the login nodes when connecting with the RemoteSSH extension.  Even basic tasks like editing code with Intellisense can flag users for login node abuse.  The current method to connect to a compute node with VSCode requires creating an interactive job with the scheduler, identifying the compute node you are placed on, and then launching a new window to this node that will likely be different with the next job submission.  This script attempts to make requesting a compute node for VSCode easier and incentivize users to quickly move from the login node to a compute node when they are executing code, navigating large file spaces, need an exclusive node, or need additional compute, memory, or GPU capability from within VSCode.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#procedure","title":"Procedure","text":"<ol> <li>Open VSCode on your local machine</li> <li>In VSCode, connect to a Casper login node using RemoteSSH as described here</li> <li>Once connected to the login node, open up a new terminal window (Ctrl+Shift+`)</li> <li>Load the ncarenv/24.12 using <code>module load ncarenv/24.12</code> in the terminal</li> <li>Call <code>qvscode</code></li> <li>Enter a valid project code and follow the prompts to launch a PBS job</li> <li>A new VSCode window will open and connect to the compute node when your PBS job has started</li> </ol> <p>A new VSCode window will launch on the compute node after user input or reading from the settings file.  You will need to enter your NCAR CIT password and provide DUO authentication when connecting to the compute node. The above procedure can be simplified by using a settings file as described in the Settings Mode section.</p> <p>Info</p> <p>Step 6 will not prompt you for a project code if you have PBS_ACCOUNT defined.</p> <p>Warning</p> <p>You must have a running VSCode Casper login session to launch the qvscode script. Connect to a login node and then launch the qvscode script from VSCode's built-in terminal.  This script will not work if you are connected to your local machine or launch it from outside of VSCode.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#operating-modes","title":"Operating Modes","text":"<p>Prompt Mode will prompt the user for the values needed to launch the PBS job.  It contains default values to make launching a job faster.</p> <p>Settings Mode reads in variables from a user defined settings file and quickly launches a compute node with these settings.  It requires a specific path and format.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#prompt-mode","title":"Prompt Mode","text":"<p>This mode will be used when the script does not find the file <code>.qvscode_settings</code> in your home directory or you use the bypass argument.  You will be prompted for the PBS select statement arguments. </p> <pre><code>bneuman@casper-login2:~&gt; qvscode\nSubmitting job to Casper\nEnter Project []: \n</code></pre> <p>If you do not have a variable <code>PBS_ACCOUNT</code> setup then you will always be prompted to enter a valid project.  After the project prompt you will be asked if you would like to use the default values for the PBS select statements.  Answering 'N' to the default values prompts the user to enter variables for each of these basic job settings.  Note that the bracketed values are the default values and will be used if you do not enter any value when prompted.  </p> <p>The defaults for the basic settings are for a serial CPU session with values of:</p> <pre><code>Account:  $PBS_ACCOUNT\nNodes:    1         \nCPUs:     1         \nMemory:   10GB         \nGPUs:     0         \nWalltime: 02:00:00         \nPath:     $(pwd)\n</code></pre> <p>After finishing the basic settings you will be prompted to enter advanced options.  The advanced options are:</p> <pre><code>CPU Type:    \nGPU Type:    \nMPI Procs:   \nOMP Threads: \n</code></pre> <p>The advanced options have no default values and will only accept CPU and GPU types that are defined in PBS.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#settings-mode","title":"Settings Mode","text":"<p>If the <code>qvscode</code> script finds a <code>.qvscode_settings</code> file in <code>$HOME</code> then it will import the variables into the script. The template requires specific keywords to pull values in.  It is highly recommended to copy the repository's <code>qvscode_settings_template</code> to your <code>$HOME</code> directory, rename it to <code>.qvscode_settings</code>, and then modify the values for each argument instead of manually creating the settings file.  You can use this command to copy the existing template into your home directory:</p> <p><code>cp /glade/work/csgteam/qvscode/qvscode_settings_template $HOME/.qvscode_settings</code></p> <p>The format for the template:</p> <pre><code>system=casper\nproject=SCSG0001\nnodes=1\nnum_cpus=1\ncpu_type=\nmemory=4GB\nmpi_procs=1\nompthreads=1\nnum_gpus=\ngpu_type=\nwalltime=01:00:00\npath=$(pwd)\n</code></pre> <p>The <code>qvscode</code> script will use the default values for any variable that is not set.  If the path variable is empty then you will create a VSCode session that does not connect to GLADE and you won't be able to use the File Explorer but can still navigate the filesystem from the terminal.</p> <p>As an example, if you wanted to request a single node of four Casper 80GB A100s then your settings would look like this:</p> <pre><code>system=casper\nproject=SCSG0001\nnodes=1\nnum_cpus=4\ncpu_type=milan\nmemory=200GB\nmpi_procs=4\nompthreads=1\nnum_gpus=4\ngpu_type=a100\nwalltime=01:00:00\npath=$(pwd)\n</code></pre> <p>With your modified project code and memory requirements.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#bypass-and-custom-settings-file-path-modes","title":"Bypass and Custom Settings File Path Modes","text":"<p>You can bypass any existing <code>$HOME/.qvscode_settings</code> file by adding the <code>-b</code> or <code>--bypass</code> argument when calling <code>qvscode</code>.  You will then enter Prompt Mode.</p> <p>You can use a custom settings file by providing the full path to the file with the <code>-p</code> or <code>--path</code> argument when calling <code>qvscode</code>.  Your custom .qvscode_settings file must use the same template format to set the values correctly.</p> <pre><code>Usage: qvscode [options]\n  options:\n    -b | --bypass      # Use prompt mode to enter job arguments and bypass any user settings files\n    -p | --path path   # Provide the full path to a qvscode settings file to use in settings mode\n    -h | --help        # Display this help message\n</code></pre>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#the-new-vscode-compute-node-job","title":"The New VSCode Compute Node Job","text":"<p>If the qsub arguments are valid then a new VSCode window will launch and connect to the compute node hostname.  You will be required to enter your NCAR two-factor authentication and may be asked to verify the SSH connection if you have not connected to the node previously.  You can close the new window and reconnect using the command provided by the script at job launch from the login node.  The format will be:</p> <p><code>code --remote ssh-remote+$USER@&lt;computenode_hostname&gt;.hpc.ucar.edu</code></p> <p>Pressing <code>Ctrl+C</code> from the login node terminal will kill the PBS job and end your compute node session.</p>"},{"location":"blog/launch-vscode-on-casper-compute-nodes/#log-files-and-other-considerations","title":"Log Files and Other Considerations","text":"<p>Log files are stored in <code>$SCRATCH/.qvscode_logs</code> and show the user arguments and job submission details.</p> <p>This script will only work on Casper due to security guidelines set for Derecho's compute nodes.  We're investigating ways to connect to Derecho compute nodes using VSCode but there is no timeline.</p> <p>Please pass along any errors or improvements to NHUG Slack channel.</p>"},{"location":"nhug/featured/","title":"Featured Projects","text":"<p>Coming Soon!</p>"},{"location":"nhug/resources/","title":"NHUG Resources","text":""},{"location":"nhug/resources/#nhug-slack-channel","title":"NHUG Slack Channel","text":"<p>Join our NHUG Slack Channel to connect and collaborate with other NHUG members. We will use this channel for focussed interactions during and outside the monthly meetings. It's the go-to spot for sharing insights, asking questions, or just chatting about all things HPC at NCAR.</p> <p>Join NHUG Slack Channel</p>"},{"location":"nhug/resources/#nhug-email-list","title":"NHUG Email List","text":"<p>Join NHUG email list to receive calendar invites for upcoming monthly meetings and other important announcements.</p> <p>Join NHUG Email List</p>"},{"location":"nhug/resources/#ncar-hpc-documentation","title":"NCAR HPC Documentation","text":"<p>The NCAR HPC Documentation site provides information about the NCAR HPC systems, including Derecho, Casper, and GLADE. It also provides information about the software and tools available on these systems, as well as information about how to use them.</p> <p>NCAR HPC Docs</p>"},{"location":"nhug/resources/#consulting-services-office-hours","title":"Consulting Services Office Hours","text":"<p>Looking for one-on-one help? Our Consulting Services are here for you. Book a session during our Office Hours for personalized support with any HPC-related challenges. Just click below to find a time that works for you.</p>"},{"location":"nhug/resources/#nhug-events-calendar","title":"NHUG Events Calendar","text":""},{"location":"nhug/archive/","title":"Meeting Notes","text":"<p>We keep all the meeting agenda, slides (if applicable), minutes and links to recordings with dates (in reverse chronology order). Please click the side-menu-bar with  appropriate dates.</p> <p>Latest Agenda: 2024-01-16</p>"},{"location":"nhug/archive/2022-11-01/","title":"2022-11-01","text":""},{"location":"nhug/archive/2022-11-01/#agenda","title":"Agenda","text":"<ul> <li>November 2022 Events</li> <li>NCAR HPC 1-year outlook:<ul> <li>Gust and Derecho Status</li> <li>Derecho, Cheyenne, and Storage Transitions</li> <li>NOAA / NCAR GPU Hackathon</li> <li>Casper</li> </ul> </li> </ul>"},{"location":"nhug/archive/2022-11-01/#meeting-slides","title":"Meeting Slides","text":""},{"location":"nhug/archive/2022-12-06/","title":"2022-12-06","text":""},{"location":"nhug/archive/2022-12-06/#agenda","title":"Agenda","text":"<p>Below is the agenda for this meeting:</p> <ul> <li>Upcoming Events<ul> <li>December 2022</li> <li>January 2023: HPC System Maintenance</li> </ul> </li> <li>Derecho Status</li> <li>NCAR HPC Usage: 2022 Year-in-Review</li> </ul>"},{"location":"nhug/archive/2022-12-06/#meeting-slides","title":"Meeting Slides","text":""},{"location":"nhug/archive/2023-02-07/","title":"2023-02-07","text":""},{"location":"nhug/archive/2023-02-07/#agenda","title":"Agenda","text":"<ul> <li>Welcome</li> <li>Upcoming Events</li> <li>NCAR/NOAA GPU Hackathon on  White House OSTP Announcement</li> <li>Upcoming Changes: CPU and GPU Accounting</li> <li>HPC System Maintenance Update</li> <li>NWSC-3 Project Status</li> <li>Virtual Consulting Services</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-02-07/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-03-07/","title":"2023-03-07","text":""},{"location":"nhug/archive/2023-03-07/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NWSC-3 Project Status</li> <li>Upcoming Events</li> <li>Updates from NCAR/NOAA GPU Hackathon</li> <li>CPU and GPU Accounting Changes Implemented!</li> <li>New, improved web presence for RDA</li> <li>New Tutorial Published (Using Dask on HPC Systems)</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-03-07/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-04-04/","title":"2023-04-04","text":""},{"location":"nhug/archive/2023-04-04/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>NWSC-3 Project Status</li> <li>Upcoming Events</li> <li>Upcoming Outages</li> <li>Derecho Documentation Update</li> <li>Using CI/CD on HPC at NCAR</li> <li>Community Input Casper Augmentation</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-04-04/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-05-02/","title":"2023-05-02","text":""},{"location":"nhug/archive/2023-05-02/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>NWSC-3 Project Status</li> <li>Upcoming Events</li> <li>Upcoming Outages</li> <li>Derecho Documentation Update</li> <li>Using CI/CD on HPC at NCAR</li> <li>Community Input Casper Augmentation</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-05-02/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-06-06/","title":"2023-06-06","text":""},{"location":"nhug/archive/2023-06-06/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NWSC-3 Project Status</li> <li>Derecho Availability Testing </li> <li>Upcoming Events</li> <li>Cheyenne Status </li> <li>Glade 2 Transition</li> <li>NHUG Survey Request</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-06-06/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-07-11/","title":"2023-07-11","text":""},{"location":"nhug/archive/2023-07-11/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NWSC-3 Project Status</li> <li>Derecho Computing Environment</li> <li>Derecho Early Users Stories</li> <li>Upcoming Events</li> <li>Cheyenne Status and Outage</li> <li>Campaign Storage on Cheyenne</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-07-11/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-09-05/","title":"2023-09-05","text":""},{"location":"nhug/archive/2023-09-05/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NWSC-3 Project Completion Status and Glade 2 Transition</li> <li>Derecho Early Users Stories</li> <li>Upcoming Events</li> <li>Cheyenne Status and Outage</li> <li>Upcoming Casper Changes</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-09-05/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-10-04/","title":"2023-10-04","text":""},{"location":"nhug/archive/2023-10-04/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NWSC-3 Project Completion Status and Glade 2 Transition</li> <li>Derecho Early Users Stories</li> <li>Upcoming Events</li> <li>Cheyenne Status and Outage</li> <li>Upcoming Casper Changes</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-10-04/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-11-07/","title":"2023-11-07","text":""},{"location":"nhug/archive/2023-11-07/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Announcements</li> <li>Glade Transition - Past and Future</li> <li>Casper Updates</li> <li>JupyterHub Updates</li> <li>Cron Workflows</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-11-07/#slides","title":"Slides","text":""},{"location":"nhug/archive/2023-12-05/","title":"2023-12-05","text":""},{"location":"nhug/archive/2023-12-05/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Announcements</li> <li>Glade Transition - Important Updates</li> <li>Distributed Machine Learning on Derecho GPUs</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2023-12-05/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-01-16/","title":"2024-01-16","text":"<p>Add this meeting to your calendar.</p>"},{"location":"nhug/archive/2024-01-16/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>NHUG 2024 Updates</li> <li>Upcoming Events and Opportunities</li> <li>Upcoming outages</li> <li>JupyterHub Updates from Jan, 9<sup>th</sup> Outage</li> <li>Glade FS1 final migration steps</li> <li>New Cron Solution</li> <li>Using Containers on HPC</li> <li>Announcing New HPC-Docs!</li> </ul>"},{"location":"nhug/archive/2024-01-16/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-02-20/","title":"2024-02-20","text":"<p>Add this meeting to your calendar.</p>"},{"location":"nhug/archive/2024-02-20/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Updates from Derecho Outage</li> <li>Casper Augmentation Plans</li> <li>FastEddy on GPUs</li> <li>Building Modern Docs using NCAR-mkdocs Template</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2024-02-20/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-03-19/","title":"2024-03-19","text":"<p>Add this meeting to your calendar.</p>"},{"location":"nhug/archive/2024-03-19/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li><code>qhist</code> Discussions</li> <li>Using VSCode-CoPilot on NCAR HPC Systems</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2024-03-19/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-04-16/","title":"2024-04-16","text":"<p>Add this meeting to your calendar.</p>"},{"location":"nhug/archive/2024-04-16/#agenda","title":"Agenda","text":"<p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Derecho/Casper Outage Updates and Upcoming Outages</li> <li>Accelerate productivity with AI using Github CoPilot</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2024-04-16/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-05-28/","title":"2024-05-28","text":"<p>Add this meeting to your calendar.</p> <p>Please note the special time and date of this meeting.</p> <p>This meeting is scheduled for 11 am to 12 pm Mountain Time on Tuesday, May 28, 2024 due to conflict with CISL on-site activities.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Upcoming Outages</li> <li>CESM at Ultra-High Resolutions - Software Challenges</li> <li>Adding your own modules with a Spack downstream</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2024-05-28/#slides","title":"Slides","text":""},{"location":"nhug/archive/2024-06-18/","title":"2024-06-18","text":"<p>Add this meeting to your calendar.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Updates from June Outages</li> <li>New Casper GPU Nodes</li> <li>Real-time, medium-range, convection-allowing ensemble forecasts with global variable-resolution models</li> </ul>"},{"location":"nhug/archive/2024-09-17/","title":"2024-09-17","text":"<p>Add this meeting to your calendar.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Outage Updates</li> <li>Casper Augmentation Plans</li> <li>Optimizing Casper Memory Requests</li> <li>Understanding the Role of Mesoscale Atmosphere\u2013Ocean Interactions in Seasonal-to-Decadal Climate Prediction \u2013 User experience on Derecho</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2024-11-19/","title":"2024-11-19","text":"<p>Add this meeting to your calendar.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Welcome</li> <li>Upcoming Events and Opportunities</li> <li>Microscale and Mesoscale Weather and Air Quality Modeling on Derecho <ul> <li>WRF-CMAQ modeling on Derecho </li> <li>WRF-LES based weather and pollution dispersion forecasting for Paris Olympics</li> </ul> </li> <li>Using DP-SCREAM to Study Cirrus on Derecho</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2025-01-21/","title":"2025-01-21","text":"<p>Add this meeting to your calendar.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Upcoming Events and Opportunities</li> <li>Running VSCode on Casper Compute Nodes with qvscode by Brett Neuman</li> <li>CM1 Description, Recent Developments, and Future Directions by Dr. George Bryan</li> <li>Round Table</li> </ul>"},{"location":"nhug/archive/2025-02-19/","title":"2025-02-19","text":"<p>Add this meeting to your calendar.</p> <p>Below is the agenda for this NHUG meeting:</p> <ul> <li>Upcoming Events and Opportunities</li> <li>Upcoming software updates on Casper and Derecho (Brian Vanderwende)</li> <li>Scaling GPU-accelerated AI Models for Advanced Numerical Weather Prediction ( John Schreck)</li> <li>Round Table</li> </ul>"},{"location":"nhug/monthly-meetings/","title":"NHUG Monthly Meetings","text":"<p>NHUG holds monthly meetings featuring different HPC-related topics, on the third Tuesday of the month, from 1 p.m. to 2 p.m. Mountain Time. Our goal is to build a vibrant community around HPC-related issues and to increase collaboration within the NCAR HPC user community. These meetings provide a venue for discussion, coordination, and showcasing NCAR HPC-related work in progress. </p> <p>All NCAR HPC users, regardless of experience or sophistication, are welcome and encouraged to attend. </p> <p>What to Expect at NHUG Meetings:</p> <ul> <li> <p>Spotlight on Workshops and Training: We kick off every meeting by highlighting upcoming workshops and training events, ensuring you stay informed about valuable learning opportunities.</p> </li> <li> <p>NCAR HPC Updates: We provide updates on the latest NCAR HPC news and events, including system updates, maintenance, and other important information.</p> </li> <li> <p>Featured Projects: We invite members of the NCAR HPC community to share their work and experiences with the group. These talks are a great way to learn about the work of your colleagues and to get feedback on your own work.</p> </li> <li> <p>Featured Topics: We invite experts to present on a variety of topics related to HPC, including software, tools, and best practices. These talks are a great way to learn about new tools and techniques that can help you get the most out of NCAR HPC systems.</p> </li> <li> <p>Round Table: We close every meeting with an open discussion where you can ask questions, share your experiences, and get feedback from the community.</p> </li> </ul>"},{"location":"nhug/monthly-meetings/#get-involved","title":"Get Involved!","text":"<p>NHUG meetings are a great place to discuss your progress on NCAR HPC systems, get your project featured and get feedback from the community.  </p> <p>Contribute to Future NHUG Meetings</p> <p>If you are interested in giving a talk or leading a discussion or if you have suggestions for future meeting topics or ideas that can enhance our community, we'd love to hear from you. Please submit your ideas using the forms below to help shape our upcoming sessions.</p> <ul> <li> <p>Be a Presenter or Lead a Discussion</p> <p>Share your HPC-related projects or experiences with NCAR HPC systems. </p> <p> Sign up to present at future NHUG meetings.</p> </li> <li> <p>Topic Suggestion</p> <p>Have a topic you're eager to learn more about or a speaker you\u2019d like us to invite? </p> <p> Submit your ideas through our suggestion form.</p> </li> </ul>"},{"location":"nhug/monthly-meetings/#nhug-events-calendar","title":"NHUG Events Calendar","text":""},{"location":"nhug/monthly-meetings/upcoming-events/","title":"NHUG Upcoming Meetings","text":"<p>NHUG holds monthly meetings on the third Tuesday of the month, from 1 p.m. to 2 p.m. Mountain Time.</p> <p>Next NHUG Meeting : Wed, Feb 19 2025 11am\u201312pm MT</p>"},{"location":"nhug/monthly-meetings/upcoming-events/#nhug-events-calendar","title":"NHUG Events Calendar","text":"<p>You can add events to your calendar by clicking on an individual event and saving it to your Google Calendar.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/casper/","title":"Casper","text":""},{"location":"blog/category/script/","title":"Script","text":""},{"location":"blog/category/vscode/","title":"VSCode","text":""},{"location":"blog/category/energy/","title":"Energy","text":""},{"location":"blog/category/derecho/","title":"Derecho","text":""},{"location":"blog/category/example/","title":"Example","text":""},{"location":"blog/category/tips/","title":"Tips","text":""},{"location":"blog/category/post/","title":"Post","text":""},{"location":"blog/category/announcements/","title":"Announcements","text":""},{"location":"blog/category/blog/","title":"Blog","text":""}]}